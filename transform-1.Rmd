# Transform: dplyr and data manipulation

Transformation encompasses any steps you take to manipulate, reshape, refine, or transform your data. We've already touched upon some useful transformation functions in previous example code snippets, such as the `mutate` function for adding columns. This section will explore some of the most useful functionailities of the `dplyr` package, explicitly introduce the pipe operator `%>%`, and showcase how you can leverage these tools to quickly manipulate your data.

The benchmark `dplyr` functions are :

  - `mutate()` to create new columns/variables from existing data
  - `arrange()` to reorder rows 
  - `filter()` to refine observations by their values (in other words by row)
  - `select()` to pick variables by name (in other words by column)
  - `summarize` to collapse many values down to a single summary. 
  
We'll go through each of these functions, but we highly recommend you read [Chapter 3: Data Transformation](https://r4ds.had.co.nz/transform.html) from *R for Data Science* to get a more comprehensive breakdown of these functions. Note that the information here is based on a `tidyverse` approach, but this is only one way of doing things. Please see the [Further reading] section for links to other equally suitable approaches to data transformation. 

Let's explore the functionality of `dplyr` using some flame absorption/emission spectroscopy (FAES) data from a *CHM317* lab. This data represents the emission signal of five sodium (Na) standards measured in triplicate:

```{r, message = FALSE}
# Importing using tips from Import chapter
FAES <- read_csv(file = "data/FAESdata.csv")

FAES <- pivot_longer(data = FAES,
    cols = -std_Na_conc,
    names_to = "replicate", 
    names_prefix = "reading_",
    values_to = "signal")

FAES <- separate(data = FAES,
    col = std_Na_conc,
    into = c("type", "conc_Na", "units"),
    sep = " ",
    convert = TRUE)

DT::datatable(FAES)
```

**Note** the use of `convert = TRUE` in the `separate()` call. This runs a type convert on new columns. If we didn't include this, the `conc_Na` column would be of type character because the numbers originated from a string. `convert()` ensures they're converted to numeric. **Always use `convert = TRUE`** when you separate columns. 

## Selecting by row or value

`filter()` allows up to subset our data based on observation (row) values. 

```{r}
filter(FAES, conc_Na == 0)
```

Note how we need to pass logical operations to `filter()` to tell it which rows to extract. In the above code, we used `filter()` to get all rows where the concentration of sodium is equal to 0 (`== 0`). Note the presence of two equal signs (`==`). In R one equal sign (`=`) is used to pass an argument, two equal signs (`==`) is the logical operation "is equal" and is used to test equality (i.e. that both sides have the same value). A frequent mistake is to use `=` instead of `==` when testing for equality. 

### Logical operators

`filter()` can use other *relational* and *logical* operators or combinations thereof. Relational operators compare values and logical operators carry out Boolean operations (TRUE or FALSE). Logical operators are used to combine multiple relational operators... let's just list what they are and how we can use them:

```{r, echo = FALSE}

ops <- data.frame("Operator" = c(">", "<","<=",">=","==","!=", "&","!","|", "is.na()"),
                  "Type" = c("relational", "relational", "relational", "relational", "relational", "relational", "logical", "logical", "logical", "function"), 
                  Description = c("Less than", "Greater than", "Less than or equal to", "Greater than or equal to", "Equal to", "Not equal to", "AND", "NOT", "OR", "Checks for missing values, TRUE if NA" ))


knitr::kable(ops)
```

- Selecting all signals below a threshold value

```{r}
filter(FAES, signal < 4450)
```

- Selecting signals between values 

```{r}
filter(FAES, signal >= 4450 & signal < 8150)
```

- Selecting all other replicates other than replicate `2`

```{r}
filter(FAES, replicate != 2)
```

- selecting the first standard replicate OR any of the blanks.

```{r}
filter(FAES, (type == "standard" & replicate == 1) | (type == "blank"))
```
- removing any missing values (`NA`) using `is.na()`. Note there are no missing values in our data set so nothing will be removed, if we removed the NOT operator (`!`) we would have selected all rows *with* missing values. 

```{r}
filter(FAES, !is.na(signal))
```



These are just some examples, but you can combine the logical operators in any way that works for you. Likewise, there are multiple combinations that will yield the same result, it's up to you do figure out which works best for you. 

## Arranging rows

`arrange()` simple reorders the rows based on the value you passed to it. By default it arranges the specified values into ascending order. Let's arrange our signal in increasing by increasing order:

```{r}
arrange( FAES, signal)
```

Since our original `FAES` data is already arranged by increasing `cong_Na` and `replicate`, let's inverse that order by arranging `conc_Na` into descending order using the `desc()` function BUT arrange the `signal` values in:

```{r}
# Note the order of precedence
arrange(FAES, desc(conc_Na), signal)
```

Just note with `arrange()` that `NA` values will always be placed at the bottom, whether you use `desc()` or not. 

## Selecting by column or variable

`select()` allows you to readily select columns by name. Note however that it will always return a tibble, even if you only select one variable/column.

```{r}
select(FAES, signal)
```
You can also select multiple columns using the same helper functions described in [Tidying Your Data]. 

```{r}
select(FAES, -units)
```
```{r}
select(FAES, starts_with("conc"))
```
-Using a colon `:` selects all columns between two provided header names (inclusive)
```{r}
select(FAES, conc_Na:replicate)
```

```{r}
# Getting columns where the header contains the character "p"
select(FAES, contains("p"))
```

##  Adding new variables

`mutate()` allows you to add new variable (read columns) to your existing data set. It'll probably be the workhorse function you'll use during your data transformation as you can readily pass other functions and mathematical operators to it to transform your data. let's suppose that our standards were diluted by a factor of 10, we can add a new column for this:

```{r}
mutate(FAES, "dil_fct" = 10)
```

We can also create multiple columns in the same `mutate()` call: 

```{r}
mutate(FAES, "dil_fct" = 10, "adj_signal" = signal * dil_fct)
```


Couple of things to note: 

  1. The variable we're creating needs to be in quotation marks, hence `"dil_fct"` for our dilution factor variable
  2. the variables we're referencing do not need to be in quotation marks; hence `signal` because this variable already exist. 
  3. Note the order of precedence: `dil_fct` is created first so we can reference in the second argument, we would get an error if we swapped the order. 
  
### Useful mutate function

There are a myriad of functions you can make use of with the mutate function. Here are some of the mathematical operators available in R: 

```{r, echo = FALSE}

funs <- data.frame("function" = c("+", "-", "*", "/", "^", "log()", "round()"),
                   "definition" = c("additon", "subtraction", "multiplication", "division", "exponent; to the power off...", "returns the specified base-log; see also log10() and log2()", "rounds to a specified number of decimal places"))

knitr::kable(funs)
```
```{r}
FAES_ex <- select(FAES, signal)
mutate(FAES_ex, "rounded_signal" = round(signal, digits = 1), "log10_signal" = log10(signal), "log2_signal" = log2(signal), "doubled_signal" = signal + signal)
```



## Group and summarize data

`summarize` effectively summarizes your data based on functions you've passed to it (and uses a similar syntax to the `mutate` function to create new columns). Looking at our `FAES` data we'd probably want the mean of the triplicate signals, alongside the standard deviation. Let's see what happens when we apply the summarize function straight up. 

```{r}
FAES
summarize(FAES, "mean" = mean(signal), "stdDev" = sd(signal))
```

This doesn't look like what we wanted. What we got was the mean and standard deviation of *all* of the signals, including standards and blanks. Also note how we've lost the other columns/variables and are only left with the mean and stdDev. This is all because we need to **group** our observations by a variable. We can do this by using the `group_by()` function.

```{r}
FAES
groupedFAES <- group_by(FAES, type)
summarize(groupedFAES, "mean" = mean(signal), "stdDev" = sd(signal))
```
Essentially, *grouping* a tibble invisibly organizes it into 1 or more sub-tables. The summarize and mutate functions will then apply their operations to each group seperately. This is why the summarize function produced a separate `mean` for each of our two groups. Grouping is a very powerful data organizing technique which will make it much easier to quickly organize and perform operations on large datasets.

Still, it would be nice if we could also get separate mean and stdDev values for each concentration of standard as well. Let's try grouping by both `type` and `conc_Na`.

```{r}
FAES
groupedFAES <- group_by(FAES, type, conc_Na)
summarize(groupedFAES, "mean" = mean(signal), "stdDev" = sd(signal))
```

Here we've created a new data set, `groupedFAES`, that we grouped by the variables `type` and `conc_Na` so we could get the mean and standard deviation of each group. Note the multiple levels of grouping. This new tibble has a total number of rows equal to the number of unique group combinations. For this data set we could have omitted the `type` variable, but in larger datasets you may have multiple groupings (e.g. different sampling locations), so you can group by multiple variables to get smaller groups. 

The other benefit to using multiple groups is that the summarize function automatically retains grouping variables in the new summary table. If we only grouped the data by `conc_Na` but still wanted to retain the `type` column in our summary table, we would have to declare it separately:

```{r}
FAES
groupedFAES <- group_by(FAES, conc_Na)
summarize(groupedFAES, "type" = unique(type), "mean" = mean(signal), "stdDev" = sd(signal))
```
We used the `unique()` function with the `type` column so that the number of rows would be less than or equal to the number of groups. When there is a mismatch in the number of rows across the columns being summarized, then the values in the shorter columns are duplicated within groups until there are an equal number of rows in each column. Try the above code without the `unique` function (`"type" = type`) and you'll see what I mean.

### Useful summarize functions

We've used the `mean()` and `sd()` functions above, but there are a host of other useful functions you can use in conjunction with summarize. See **Useful Functions** in the `summarise()` documentation (enter `?summarise`) in the console. 

## The pipe: chaining functions together 

With the tools presented here we could do a decent job analyzing our `FAES` data. Let's say we wanted to subtract the mean of the `blank` from each `standard` signal and then get summarize those results. It would look something like this: 

```{r}
blank <- filter(FAES, type == "blank")
meanBlank <- summarize(blank, mean(signal))
meanBlank <- as.numeric(meanBlank)

paste("The mean signal from the blank triplicate is:", meanBlank)

stds_1 <- filter(FAES, type == "standard")
stds_2 <- mutate(stds_1, "cor_sig" = signal - meanBlank)
stds_3 <- group_by(stds_2, conc_Na)
stds_4 <- summarize(stds_3, "mean" = mean(cor_sig), "stdDev" = sd(cor_sig))
stds_4
```

While the code above did it's job, it's certainly wasn't easy to type and certainly not easy to read. At every step of the way we've saved our updated data outputs to a new variable (`stds_1`, `stds_2`, etc.). However, most of these intermediates aren't important, and moreover the repetitive names clutter our code. As the code above is written, we've had to pay special attending to the variable suffix to make sure we're calling the correct data set as our code has progresses. An alternative would be to reassign the outputs back to the original variable name (i.e. `stds_1 <- mutate(stds_1, ...)`), but that doesn't solve the issue of readability as there's still redundant assigning. 

A solution for this is the pipe operator `%>%` ( pronounced "then"), an incredibly useful tool for writing more legible and understandable code. The pipe basically changes how you read code to emphasize the functions you're working with by passing the intermediate steps to hidden processes in the background. Re-writing the code above, we'd get something like: 

```{r}
meanBlank <- FAES %>%
  filter(type =="blank") %>%
  summarise(mean(signal)) %>%
  as.numeric()

paste("The mean signal from the blank triplicate is:", meanBlank)
```

Things may look a bit different, but our underlying code hasn't changed much. What's happening is the pipe operator passes the output to the first argument of the next function. So the output of `filter...` is passed to the first argument of `sumamrise...`, and the argument we specified in `summarise` is actually the *second* argument it receives. You're probably wondering how hiding stuff makes your code more legible, but think of `%>%` as being equivalent to "then". We can read our code as:

>"Take the `FAES` dataset, *then* filter for `type == "blank"` *then* collapse the dataset to the mean `signal` value and *then* convert to numeric value *then* pass this final output to the new variable `meanBlank`." 

Not only is the pipe less typing, but the emphasis is on the functions so you can better understand what you're doing vs. where all the intermediates are going. Extending our piping to the second batch of code we get:  


```{r}
stds <- FAES %>%
  filter(type == "standard") %>%
  mutate("cor_sig" = signal - meanBlank) %>% 
  group_by(conc_Na) %>%
  summarize("mean" = mean(cor_sig), "stdDev" = sd(cor_sig))

stds
```

Same thing. The underlying code hasn't changed much, but it's much more legible and we can clearly see we're subtracting the `meanBlank` value from each measured signal then summarizing the corrected signals. 

### Notes on piping

The pipe is great and especially useful with *tidyverse* packages, but it does have some limitations: 

  - You can't easily extract intermediate steps. So you'll need to break up your pipping chain to output any intermediate steps you can. 
  - The benefit of piping is legibility; this goes away as you increase the number of steps as you lose track of what's going on. Keep the piping short and thematically similar. 
  - Pipes are linear, if you have multiple inputs or outputs you should consider an alternative approach. 


## Further reading

  - [Chapter 5: Data Transformation](https://r4ds.had.co.nz/transform.html) of *R for Data Science* for a deeper breakdown of `dplyr` and it's functionality. 
  - [Chapter 18: Pipes](https://r4ds.had.co.nz/pipes.html) of *R for Data Science* for more information on pipes. 
  - [Syntax equivalents: base R vs Tidyverse](https://tavareshugo.github.io/data_carpentry_extras/base-r_tidyverse_equivalents/base-r_tidyverse_equivalents.html) by Hugo Taveres for a comparison of base-R solutions to tidyverse. This entire book is largely biased towards tidyverse solutions, but there's no denying that certain base-R can be more elegant. Check out this write up to get a better idea. 