# Transforming Your Data

Transformation encompasses any steps you take to manipulate, reshape, refine, or transform your data. We've already touched upon some useful transformation functions in previous example code snippets, such as the `mutate` function for adding columns. This section will explore some of the most useful functionailities of the `dplyr` package, explicitly introduce the pipe operator `%>%`, and showcase how you can leverage these tools to quickly manipulate your data.

The essential `dplyr` functions are:

  - `mutate()` to create new columns/variables from existing data
  - `arrange()` to reorder rows 
  - `filter()` to refine observations by their values (in other words by row)
  - `select()` to pick variables by name (in other words by column)
  - `summarize()` to collapse many values down to a single summary. 
  
We'll go through each of these functions, but we highly recommend you read [Chapter 3: Data Transformation](https://r4ds.had.co.nz/transform.html) from *R for Data Science* to get a more comprehensive breakdown of these functions. Note that the information here is based on a `tidyverse` approach, but this is only one way of doing things. Please see the [Further reading] section for links on other approaches to data transformation. 

Let's explore the functionality of `dplyr` using some flame absorption/emission spectroscopy (FAES) data from a *CHM317* lab. This data represents the emission signal of five sodium (Na) standards measured in triplicate:

```{r, message = FALSE}
# Importing using tips from Import chapter
FAES <- read_csv(file = "data/FAESdata.csv")

FAES <- pivot_longer(data = FAES,
    cols = -std_Na_conc,
    names_to = "replicate", 
    names_prefix = "reading_",
    values_to = "signal")

FAES <- separate(data = FAES,
    col = std_Na_conc,
    into = c("type", "conc_Na", "units"),
    sep = " ",
    convert = TRUE)

DT::datatable(FAES)
```

**Note** the use of `convert = TRUE` in the `separate()` call. This runs a type convert on new columns. If we didn't include this, the `conc_Na` column would be of type character because the numbers originated from a string. `convert()` ensures they're converted to numeric. **Always use `convert = TRUE`** when you separate columns. 

## Extracting rows by value

`filter()` allows up to subset our data based on observation (row) values. 

```{r}
filter(FAES, conc_Na == 0)
```

Note how we need to pass logical operations to `filter()` to tell it which rows to extract. In the above code, we used `filter()` to get all rows where the concentration of sodium is equal to 0 (`== 0`). Note the presence of two equal signs (`==`). In R one equal sign (`=`) is used to pass an argument, two equal signs (`==`) is the logical operation "is equal" and is used to test equality (i.e. that both sides have the same value). A frequent mistake is to use `=` instead of `==` when testing for equality. 

### Logical operators

`filter()` can use other *relational* and *logical* operators or combinations thereof. Relational operators compare values and logical operators carry out Boolean operations (TRUE or FALSE). Logical operators are used to combine multiple relational operators... let's just list what they are and how we can use them:

```{r, echo = FALSE}

ops <- data.frame("Operator" = c(">", "<","<=",">=","==","!=", "&","!","|", "is.na()"),
                  "Type" = c("relational", "relational", "relational", "relational", "relational", "relational", "logical", "logical", "logical", "function"), 
                  Description = c("Less than", "Greater than", "Less than or equal to", "Greater than or equal to", "Equal to", "Not equal to", "AND", "NOT", "OR", "Checks for missing values, TRUE if NA" ))


knitr::kable(ops)
```
Here's a series of examples showing some different uses of the operators above:

- Selecting all signals below a threshold value

```{r}
filter(FAES, signal < 4450)
```

- Selecting signals between values 

```{r}
filter(FAES, signal >= 4450 & signal < 8150)
```

- Selecting all other replicates other than replicate `2`

```{r}
filter(FAES, replicate != 2)
```

- Selecting the first standard replicate OR any of the blanks

```{r}
filter(FAES, (type == "standard" & replicate == 1) | (type == "blank"))
```
- Removing any missing values (`NA`) using `is.na()`. Note there are no missing values in our data set so nothing will be removed, if we removed the NOT operator (`!`) we would have selected all rows *with* missing values. 

```{r}
filter(FAES, !is.na(signal))
```



These are just some examples, but you can combine the logical operators in any way that works for you. Likewise, there are multiple combinations that will yield the same result, it's up to you do figure out which works best for you. 

## Reordering rows

`arrange()` simply reorders the rows of a tibble based on the value you passed to it. By default it arranges the specified values into ascending order. Let's arrange our data by increasing order of signal value:

```{r}
arrange( FAES, signal)
```

Since our original `FAES` data is already arranged by increasing `cong_Na` and `replicate`, let's inverse that order by arranging `conc_Na` into descending order using the `desc()` function. Within that, we'll also arrange the `signal` values in ascending order:

```{r}
# Note the order of precedence (left-to-right)
arrange(FAES, desc(conc_Na), signal)
```
This is possible here because there are duplicate values of `conc_Na`. Just note that when using `arrange()`, `NA` values will always be placed at the bottom whether you use `desc()` or not. 

## Extracting columns by name

`select()` allows you to readily select columns by name. Note that it will always return a tibble, even if you only select one column.

```{r}
select(FAES, signal)
```
You can also select multiple columns using the same helper functions and operators described in [Tidying Your Data]:

- Selecting all columns except one

```{r}
select(FAES, -units)
```

- Selecting only columns with names that start with "conc"
```{r}
select(FAES, starts_with("conc"))
```
- Selecting all columns from `conc_Na` to `replicate`
```{r}
select(FAES, conc_Na:replicate)
```
- Selecting all columns where the header contains the character "p"
```{r}
select(FAES, contains("p"))
```

##  Adding and transforming columns

`mutate()` allows you to add new columns (i.e. variables) to your existing data set. It'll probably be the most used function for your data transformation needs as you can readily pass other functions and mathematical operators to it to transform your data. Let's suppose that our standards were diluted by a factor of 10, we can add a new column for this:

```{r}
mutate(FAES, "dil_fct" = 10)
```

We can also create multiple columns in the same `mutate()` call: 

```{r}
mutate(FAES, "dil_fct" = 10, "adj_signal" = signal * dil_fct)
```

<!-- Is this just to help them keep track of what's what? As far as I can tell, the quotation marks are unnecessary. I definitely don't use them. -->
<!-- Couple of things to note: 

  1. The variable we're creating needs to be in quotation marks, hence `"dil_fct"` for our dilution factor variable
  2. The variables we're referencing do not need to be in quotation marks; hence `signal` because this variable already exists. -->
Note the left-to-right order of precedence: `dil_fct` is created first, which allows us to reference it in the second argument. We would get an error if we swapped the order. 
  
### Useful math functions

There are a myriad of mathematical functions and operators you can make combine with the mutate function. Here are some of the most common ones you might need: 

```{r, echo = FALSE}

funs <- data.frame("Function/Operator" = c("+", "-", "*", "/", "^", "log()", "round()"),
                   "Definition" = c("Addition", "Subtraction", "Multiplication", "Division", "Exponent; to the power of...", "Returns the specified base-log; see also log10() and log2()", "Rounds to a specified number of decimal places"))

knitr::kable(funs)
```

Let's look at an example combining `select()` and `mutate()` to generate columns containing different transformations of the same data.
```{r}
FAES_ex <- select(FAES, signal)
mutate(FAES_ex, "rounded_signal" = round(signal, digits = 1), "log10_signal" = log10(signal), "log2_signal" = log2(signal), "doubled_signal" = signal + signal)
```

You can even use `mutate()` to transform data without adding a new column! 
```{r}
mutate(FAES, conc_Na = conc_Na*1000, units = "ug/L", "signal_string" = as.character(signal))
```
Here we've changed our units to be in micrograms per litre. This also demonstrates how to easily change the data type of a column, though you'll use this more often to convert numbers which were incorrectly imported as strings back into numeric type.

## Grouping and summarizing data

`summarize()` effectively summarizes your data based on functions you've passed to it (and uses a similar syntax to the `mutate` function to create new columns). Looking at our `FAES` data we'd probably want the mean of the triplicate signals, alongside the standard deviation. Let's see what happens when we apply the summarize function straight up. 

```{r}
summarize(FAES, "mean" = mean(signal), "stdDev" = sd(signal))
```

This doesn't look like what we wanted. What we got was the mean and standard deviation of *all* of the signals, including standards and blanks. Also note how we've lost the other columns/variables and are only left with the mean and stdDev. This is all because we need to **group** our observations by a variable. We can do this by using the `group_by()` function.

```{r}
groupedFAES <- group_by(FAES, type)
summarize(groupedFAES, "mean" = mean(signal), "stdDev" = sd(signal))
```
Essentially, *grouping* a tibble invisibly organizes it into 1 or more sub-tables. The summarize and mutate functions will then apply their operations to each group separately. This is why the summarize function produced a separate `mean` for each of our two groups. Grouping is a very powerful data organizing technique which will make it much easier to quickly organize and perform operations on large datasets.

Still, it would be nice if we could also get separate mean and stdDev values for each concentration of standard as well. Let's try grouping by both `type` and `conc_Na`.

```{r}
groupedFAES <- group_by(FAES, type, conc_Na)
summarize(groupedFAES, "mean" = mean(signal), "stdDev" = sd(signal))
```

Here we've created a new data set, `groupedFAES`, that we grouped by the variables `type` and `conc_Na` so we could get the mean and standard deviation of each group. Note the multiple levels of grouping. This new tibble has a total number of rows equal to the number of unique group combinations. For this data set we could have omitted the `type` variable, but in larger datasets you may have multiple groupings (e.g. different sampling locations), so you can group by multiple variables to get smaller groups. 

The other benefit to using multiple groups is that the summarize function automatically retains grouping variables in the new summary table. If we only grouped the data by `conc_Na` but still wanted to retain the `type` column in our summary table, we would have to declare it separately:

```{r}
groupedFAES <- group_by(FAES, conc_Na)
summarize(groupedFAES, "type" = unique(type), "mean" = mean(signal), "stdDev" = sd(signal))
```
We used the `unique()` function with the `type` column so that the number of rows would be less than or equal to the number of groups. When there is a mismatch in the number of rows across the columns being summarized, then the values in the shorter columns are duplicated within groups until there are an equal number of rows in each column. Try the above code without the `unique` function (`"type" = type`) and you'll see what I mean.

We've used the `mean()` and `sd()` functions in these examples, but there are a host of other useful functions you can use in conjunction with summarize. See [Useful math functions] above, or check out *Useful Functions* in the `summarize()` documentation (enter `?summarize` in the console) for more ideas.


## The pipe: chaining functions together 

With the tools we've learned so far in this chapter I think we could do a decent job analyzing our `FAES` data now. Let's say we wanted to subtract the mean of the `blank` from each `standard` signal and then summarize those results. Normally it would look something like this: 

```{r}
blank <- filter(FAES, type == "blank") #Extract only rows containing blank samples
meanBlank <- summarize(blank, mean(signal)) #Calculate the mean of the abundance signal values from the blanks
meanBlank <- as.numeric(meanBlank) #Quick way to convert a single-cell tibble into a numeric value

paste("The mean signal from the blank triplicate is:", meanBlank) #Print a message to the console containing the mean blank signal

stds_1 <- filter(FAES, type == "standard") #Subset the FAES table to contain only standard samples
stds_2 <- mutate(stds_1, "cor_sig" = signal - meanBlank) #Create a table with a new column containing (standard signals - mean blank signal)
stds_3 <- group_by(stds_2, conc_Na) #Group rows according to the Na concentration of the standards
stds_4 <- summarize(stds_3, "mean" = mean(cor_sig), "stdDev" = sd(cor_sig)) #calculate the mean and standard deviation of each group of signal values
stds_4 #Print variable contents to console
```

While the code above did it's job, it was a chore to type and certainly isn't easy to read. At every step of this code chunk we saved our updated data outputs to a new variable (`stds_1`, `stds_2`, etc.). However, most of these intermediates aren't important, and moreover the repetitive names clutter our code. As the code above is written, we had to pay special attending to the variable suffix to make sure we're calling the correct data set as our code has progresses. An alternative would be to reassign the outputs back to the original variable name (i.e. `stds_1 <- mutate(stds_1, ...)`), but that doesn't solve the issue of readability as there are still redundant assignments happening. 

A solution for this is the pipe operator `%>%` (pronounced as "then"), an incredibly useful tool for writing more legible and understandable code. The pipe streamlines your code by makeing the actual functions stand out more, rather than the constant variable assignments or re-assignments. Let's rewrite the code above to make full use of the pipe, then we'll break down what's actually happening: 

```{r}
meanBlank <- FAES %>% 
  filter(type =="blank") %>%
  summarise(mean(signal)) %>%
  as.numeric()

paste("The mean signal from the blank triplicate is:", meanBlank)

stds <- FAES %>%
  filter(type == "standard") %>%
  mutate("cor_sig" = signal - meanBlank) %>% 
  group_by(conc_Na) %>%
  summarize("mean" = mean(cor_sig), "stdDev" = sd(cor_sig))

stds
```

Same output as before! The code may look a bit different, but the actual function usage hasn't changed at all. As mentioned above, the pipe operator always passes the output of the previous function to the first argument of the next function. So the output of `filter...` is passed to the first argument of `summarize...`, while the argument specified in `summarize` is actually the *second* argument it receives. You're probably wondering how hiding stuff makes your code more legible, but remember that `%>%` is equivalent to the word "then". This means the first chunk of code can be read as:

>"Take the `FAES` dataset, *then* filter for `type == "blank"` *then* collapse the dataset to the mean `signal` value and *then* convert to numeric value *then* pass this final output to the new variable `meanBlank`." 

Not only does this accomplish the same goal with far less typing, but the visual emphasis in the code is now on the actual functions so you can better understand what you're doing instead of worrying about where all the intermediates are going. Just note that the variable `meanBlank` is only assigned a value *after* all piped functions have been evaluated. 

The second code chunk can be read as:

>"Take the `FAES` dataset, *then* filter for `type == "standard"` *then* add a new column containing blank-subtracted signal values for the standards *then* group by `conc_Na` *then* calculate the mean and standard deviation of the blank-subtracted signal values for each group of standards *then* pass this final output to the new variable `stds`.

Again, the underlying code hasn't changed much, but it's much more legible and we can clearly see we're subtracting the `meanBlank` value from each measured signal then summarizing the corrected signals. 

### Limitations of the pipe

The pipe is great and especially useful with *tidyverse* packages, but it does have some limitations: 

  - You can't easily extract intermediate steps. So if you need to use a value or object from an intermediate step elsewhere in your code, you'll need to break up your piping chain first. 
  - The benefit of piping is legibility; this goes away as you increase the number of steps as you lose track of what's going on. Keep the piping chains short and thematically similar. 
  - Pipes are always linear, so if you have multiple inputs or outputs you should consider an alternative approach. 


## Further reading

  - [Chapter 5: Data Transformation](https://r4ds.had.co.nz/transform.html) of *R for Data Science* for a deeper breakdown of `dplyr` and it's functionality. 
  - [Chapter 18: Pipes](https://r4ds.had.co.nz/pipes.html) of *R for Data Science* for more information on pipes. 
  - [Syntax equivalents: base R vs Tidyverse](https://tavareshugo.github.io/data_carpentry_extras/base-r_tidyverse_equivalents/base-r_tidyverse_equivalents.html) by Hugo Taveres for a comparison of base-R solutions to tidyverse. This entire book is largely biased towards tidyverse solutions, but there's no denying that certain base-R can be more elegant. Check out this write up to get a better idea. 